# Simplewavenet, ch12 dil_depth 8, receptive field 512

arch: ai85simplewavenet
dataset: PEDALNET

# Define layer parameters in order of the layer sequence
layers:
#0 input_layer ai8x.Conv1d chi1o12 k3dil1
- out_offset: 0x0000 #0x2000
  processors: 0x0000000000000001
  in_dim: [1024,1]
  data_format: CHW  #better since we only have 1 channel..
  op: conv1d
  pad: 0
  kernel_size: 3
#hidden 0 ai8x.Conv1d chi12o12 k3dil1
- out_offset: 0x0000 #0x2000
  processors: 0x000000000000FFF0
  #in_dim: [1022,1]
  op: conv1d
  activate: ReLU
  dilation: 1
  pad: 0
  kernel_size: 3
#hidden 1 ai8x.Conv1d chi12o12 k3dil2
- out_offset: 0x0000 #0x2000
  processors: 0x000000000FFF0000
  op: conv1d
  activate: ReLU
  dilation: 2
  pad: 0
  kernel_size: 3
#hidden 2 ai8x.Conv1d chi12o12 k3dil4
- out_offset: 0x0000 #0x2000
  processors: 0x000000000000FFF0
  op: conv1d
  activate: ReLU
  dilation: 4
  pad: 0
  kernel_size: 3
#hidden 3 ai8x.Conv1d chi12o12 k3dil8
- out_offset: 0x0020 #izer complains because of use of hardware dilation
  processors: 0x000000000FFF0000
  op: conv1d
  activate: ReLU
  dilation: 8
  pad: 0
  kernel_size: 3
#hidden 4 ai8x.Conv1d chi12o12 k3dil16
- out_offset: 0x0040 #0x2000
  processors: 0x000000000000FFF0
  op: conv1d
  activate: ReLU
  dilation: 16
  pad: 0
  kernel_size: 3
#hidden 5 ai8x.Conv1d chi12o12 k3dil32
- out_offset: 0x0080 #0x2000
  processors: 0x000000000FFF0000
  op: conv1d
  activate: ReLU
  dilation: 32
  pad: 0
  kernel_size: 3
#hidden 6 ai8x.Conv1d chi12o12 k3dil64
- out_offset: 0x0100 #0x2000
  processors: 0x000000000000FFF0
  op: conv1d
  activate: ReLU
  dilation: 64
  pad: 0
  kernel_size: 3
#hidden 7 ai8x.Conv1d chi12o12 k3dil128
- out_offset: 0x0200 #0x2000
  processors: 0x000000000FFF0000
  op: conv1d
  activate: ReLU
  dilation: 128
  pad: 0
  kernel_size: 3

#0 linear_mix ai8x.Conv1d chi8o1 k1dil1
- out_offset: 0x0000 #0x2000
  processors: 0x000000000000FFF0
  op: conv1d
  pad: 0
  kernel_size: 1
  output_width: 32

